  В данном эксперименте на испытательном столе лежит KNN/ANN. Слабый, примитивный алгоритм. Я приношу вторую жизнь слабым алгоритмам, чувствую за них ответственность. Качаю KNN/ANN на максимум, а после сравниваю KNN по метрикам, ANN по скорости с популярными решениями из коробки. После самое лучшее решение также докручивая на максимум. Сможет ли KNN дать отпор?  
  
Классический KNN :  
1) После кодирования категориальных признаков OHE получилось 13000+ фичей: accuracy 0.37  
1.1) После уменьшения размерности - применения к OHE TruncatedSVD(n_components=500): accuracy 0.37982  
1.2) После подбор параметров у модели при помощи optuna: accuracy 0.383564  
2) Вместо OHE используем Target Encoder + подбор параметров при помощи optuna: accuracy 0.458524
2.1) Используем бэггинг + подбор параметров optuna: accuracy 0.45987753

Подводя итог -- классический KNN + бэггинг оставил след в моем эксперименте: accuracy 0.45987753  

   
