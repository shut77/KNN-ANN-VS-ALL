  В данном эксперименте на испытательном столе лежит KNN/ANN. Слабый, примитивный алгоритм. Я приношу вторую жизнь слабым алгоритмам, чувствую за них ответственность. Качаю KNN/ANN на максимум, а после сравниваю KNN по метрикам, ANN по скорости с популярными решениями из коробки. После самое лучшее решение также докручивая на максимум. Сможет ли KNN дать отпор?  
Предсказываем жанр песни. Всего жанров - 26.  

Классический KNN :  
1) После кодирования категориальных признаков OHE получилось 13000+ фичей: accuracy 0.37  
1.1) После уменьшения размерности - применения к OHE TruncatedSVD(n_components=500): accuracy 0.37982  
1.2) После подбор параметров у модели при помощи optuna: accuracy 0.383564  
2) Вместо OHE используем Target Encoder + подбор параметров при помощи optuna: accuracy 0.458524
2.1) Используем бэггинг + подбор параметров optuna: accuracy 0.45987753

Подводя итог -- классический KNN + бэггинг оставил след в моем эксперименте: accuracy 0.45987753 / 1.2 min

HNSW - модификация KNN - приближенный поиск:  
Бэггинг и TE показал  в среднем 1/2 по времени, сохраняя +- аналогичную метрику. не особо интересно. accuracy 0.4519 / 0.32 min  

Библиотек для ScaNN - нет на Window. Может потом запущу в Docker через Linux-контейнер. Хочется проверить скорость в сравнении с нашей разработкой Яндекса(HNSW).  

Catboost  
Catboost на мною сделанной предобработке данных: accuracy 0.5996347  
Catboost на мною сделанной предобработке данных, но без TE: 0.5040  
Catboost без предобработки: accuracy 0.7795037  

Как можно заметить, встроенные алгоритмы в catbooste выиграли, с почетом, уважением. Признаю, разработка сильна и успешна.  
Отстоять честь KNN в данном эксперименте не получилось.  


Анализ - почему CatBoost победил? Короткая мат. сводка:  








